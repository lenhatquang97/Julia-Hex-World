struct MDP
    Î³ # discount factor
    ğ’® # state space
    ğ’œ # action space
    T # transition function
    R # reward function
    TR # sample transition and reward
end
MDP(Î³, ğ’®, ğ’œ, T, R) = MDP(Î³, ğ’®, ğ’œ, T, R, nothing)

function transition(mdp::DiscreteMDP, s::Int, a::Int)
    return Categorical(mdp.T[s,a,:])
end

function lookahead(ğ’«::MDP, U, s, a)
    ğ’®, T, R, Î³ = ğ’«.ğ’®, ğ’«.T, ğ’«.R, ğ’«.Î³
    return R(s,a) + Î³*sum(T(s,a,sâ€²)*U(sâ€²) for sâ€² in ğ’®)
end

function lookahead(ğ’«::MDP, U::Vector, s, a)
    ğ’®, T, R, Î³ = ğ’«.ğ’®, ğ’«.T, ğ’«.R, ğ’«.Î³
    return R(s,a) + Î³*sum(T(s,a,sâ€²)*U[i] for (i,sâ€²) in enumerate(ğ’®))
end


function iterative_policy_evaluation(ğ’«::MDP, Ï€, k_max)
    ğ’®, T, R, Î³ = ğ’«.ğ’®, ğ’«.T, ğ’«.R, ğ’«.Î³
    U = [0.0 for s in ğ’®]
    for k in 1:k_max
        U = [lookahead(ğ’«, U, s, Ï€(s)) for s in ğ’®]
    end
    return U
end

function policy_evaluation(ğ’«::MDP, Ï€)
    ğ’®, R, T, Î³ = ğ’«.ğ’®, ğ’«.R, ğ’«.T, ğ’«.Î³

    # R(s, Ï€(s)) âˆ€ s âˆˆ {ğ’®}
    Râ€² = [R(s, Ï€(s)) for s in ğ’®] # nx1 = |ğ’®|x1
    Tâ€² = [T(s, Ï€(s), sâ€²) for s in ğ’®, sâ€² in ğ’®] # nxm = |ğ’®|x|ğ’®|
    return (I - Î³*Tâ€²)\Râ€²
end



struct ValueFunctionPolicy
    ğ’« # problem
    U # utility function
end

function greedy(ğ’«::MDP, U, s)
    u, a = _findmax(a->lookahead(ğ’«, U, s, a), ğ’«.ğ’œ)
    return (a=a, u=u)
end

function (Ï€::ValueFunctionPolicy)(s)
    return greedy(Ï€.ğ’«, Ï€.U, s).a
end
# -------------------------- Value Iteration --------------------------


function backup(ğ’«::MDP, U, s)
    return maximum(lookahead(ğ’«, U, s, a) for a in ğ’«.ğ’œ)
end


struct ValueIteration
    k_max # maximum number of iterations
end


function solve(M::ValueIteration, ğ’«::MDP)
    U = [0.0 for s in ğ’«.ğ’®]
    for k = 1:M.k_max
        U = [backup(ğ’«, U, s) for s in ğ’«.ğ’®]
    end
    return ValueFunctionPolicy(ğ’«, U)
end