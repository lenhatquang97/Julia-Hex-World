Base.:(==)(s1::Tuple{Int,Int}, s2::Tuple{Int,Int}) = (s1[1] == s2[1]) && (s1[2] == s2[2])
Base.:+(s1::Tuple{Int,Int}, s2::Tuple{Int,Int}) = (s1[1] + s2[1], s1[2] + s2[2])
Base.:-(s1::Tuple{Int,Int}, s2::Tuple{Int,Int}) = (s1[1] - s2[1], s1[2] - s2[2])
Base.:(==)(s1::Tuple{Tuple{Int,Int},Tuple{Int,Int}}, s2::Tuple{Tuple{Int,Int},Tuple{Int,Int}}) = (s1[1] == s2[1]) && (s1[2] == s2[2])
Base.:(==)(s1::Tuple{Tuple{Int,Int},Tuple{Int,Int},Tuple{Int,Int}}, s2::Tuple{Tuple{Int,Int},Tuple{Int,Int},Tuple{Int,Int}}) = (s1[1] == s2[1]) && (s1[2] == s2[2]) && (s1[3] == s2[3])
state =  [(0,0),(1,0),(2,0),(3,0),(0,1),(1,1),(2,1),(-1,2),
(0,2),(1,2),(2,2),(3,2),(4,2),(5,2),(6,2),(7,2),
(8,2),(4,1),(5,0),(6,0),(7,0),(7,1),(8,1),(9,0),(-100,-100)]
action =[(1,0),(0,1),(-1,1),(-1,0),(0,-1),(1,-1)]
nS = length(state) + 1
nA = 6
s_absorbing = nS
dict_T = zeros(Float64,nS,nA,nS)
dict_R = zeros(Float64,nS,nA)
r_bump_border = -1
discountFactor = 0.85
p_intended = 0.7
special_hex_rewards = Dict{Tuple{Int,Int}, Float64}(
    (0,1)=>  5.0, # left side reward
    (2,0)=>-10.0, # left side hazard
    (9,0)=> 10.0, # right side reward
)
p_veer = 0.15
function hex_neighbors(hex::Tuple{Int,Int})
    i,j = hex
    [(i+1,j),(i,j+1),(i-1,j+1),(i-1,j),(i,j-1),(i+1,j-1)]
end
function T(s,a,sâ€²)
    indexS = findfirst(k -> k == s,state)
    indexA = findfirst(k -> k == a,action)
    indexSp = findfirst(k -> k == sâ€²,state)
    return dict_T[indexS,indexA,indexSp]
end
function find(arr,s)
    for i in 1:length(arr)
        if arr[i] == s
            return i
        end
    end
    return -1
end

function R(s,a)
    indexS = findfirst(k -> k == s,state)
    indexA = findfirst(k -> k == a,action)
    return dict_R[indexS,indexA]
end


function calcTransitionAndReward(hexes::Vector{Tuple{Int,Int}},dict_T,dict_R)
    for s in 1 : length(hexes)
        hex = hexes[s]
        if !haskey(special_hex_rewards, hex)
            # Action taken from a normal tile
            neighbors = hex_neighbors(hex)
            for (a,neigh) in enumerate(neighbors)
                # Indended transition.
                sâ€² = findfirst(h -> h == neigh, hexes)
                if sâ€² == nothing
                    # Off the map!
                    sâ€² = s
                    dict_R[s,a] += r_bump_border*p_intended
                end
                dict_T[s,a,sâ€²] += p_intended

                # Unintended veer left.
                a_left = mod1(a+1, nA)
                neigh_left = neighbors[a_left]
                sâ€² = findfirst(h -> h == neigh_left, hexes)
                if sâ€² == nothing
                    # Off the map!
                    sâ€² = s
                    dict_R[s,a] += r_bump_border*p_veer
                end
                dict_T[s,a,sâ€²] += p_veer

                # Unintended veer right.
                a_right = mod1(a-1, nA)
                neigh_right = neighbors[a_right]
                sâ€² = findfirst(h -> h == neigh_right, hexes)
                if sâ€² == nothing
                    # Off the map!
                    sâ€² = s
                    dict_R[s,a] += r_bump_border*p_veer
                end
                dict_T[s,a,sâ€²] += p_veer
            end
        else
            # Action taken from an absorbing hex
            # In absorbing hex, your action automatically takes you to the absorbing state and you get the reward.
            for a in 1 : nA
                dict_T[s,a,s_absorbing] = 1.0
                dict_R[s,a] += special_hex_rewards[hex]
            end
        end
    end

    # Absorbing state stays where it is and gets no reward.
    for a in 1 : nA
        dict_T[s_absorbing,a,s_absorbing] = 1.0
    end
end
struct MDP
    Î³ # discount factor
    ğ’® # state space
    ğ’œ # action space
    T # transition function
    R # reward function
    TR # sample transition and reward
end
MDP(Î³, ğ’®, ğ’œ, T, R) = MDP(Î³, ğ’®, ğ’œ, T, R, nothing)

function lookahead(ğ’«::MDP, U, s, a)
    ğ’®, T, R, Î³ = ğ’«.ğ’®, ğ’«.T, ğ’«.R, ğ’«.Î³
    return R(s,a) + Î³*sum(T(s,a,sâ€²)*U(sâ€²) for sâ€² in ğ’®)
end

function lookahead(ğ’«::MDP, U::Vector, s, a)
    ğ’®, T, R, Î³ = ğ’«.ğ’®, ğ’«.T, ğ’«.R, ğ’«.Î³
    return R(s,a) + Î³*sum(T(s,a,sâ€²)*U[i] for (i,sâ€²) in enumerate(ğ’®))
end

struct ValueFunctionPolicy
    ğ’« # problem
    U # utility function
end

function greedy(ğ’«::MDP, U, s)
    u, a = _findmax(a->lookahead(ğ’«, U, s, a), ğ’«.ğ’œ)
    return (a=a, u=u)
end

function (Ï€::ValueFunctionPolicy)(s)
    return greedy(Ï€.ğ’«, Ï€.U, s).a
end

function backup(ğ’«::MDP, U, s)
    return maximum(lookahead(ğ’«, U, s, a) for a in ğ’«.ğ’œ)
end
struct ValueIteration
    k_max # maximum number of iterations
end


function solve(M::ValueIteration, ğ’«::MDP)
    U = [0.0 for s in ğ’«.ğ’®]
    for k = 1:M.k_max
        U = [backup(ğ’«, U, s) for s in ğ’«.ğ’®]
    end
    return ValueFunctionPolicy(ğ’«, U)
end


calcTransitionAndReward(state,dict_T,dict_R)
M = ValueIteration(10)
MD = MDP(discountFactor,state,action,T,R)
value = solve(M,MD).U