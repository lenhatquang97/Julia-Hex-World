struct MDP
    Î³ # discount factor
    ğ’® # state space
    ğ’œ # action space
    T # transition function
    R # reward function
    TR # sample transition and reward
end
MDP(Î³, ğ’®, ğ’œ, T, R) = MDP(Î³, ğ’®, ğ’œ, T, R, nothing)


#Reward
function R(s, a=missing)
	if s == (5,2)
		return -10
	elseif s == (4,1)
		return -5
	elseif s == (9,0)
		return 10
	elseif s == (7,2)
		return 3
	else
		return 0
	end
end

function T(s,a,sâ€²)
    if R(s,a) > 0
        return 1
    end
    if R(s,a) < 0
        return -0.5
    end
    return 0
end

action = [1,2,3,4,5,6]
state = [(0,0),(1,0),(2,0),(3,0),(0,1),(1,1),(2,1),(-1,2),
(0,2),(1,2),(2,2),(3,2),(4,2),(5,2),(6,2),(7,2),
(8,2),(4,1),(5,0),(6,0),(7,0),(7,1),(8,1),(9,0)]

discountFactor = 0.85

function lookahead(ğ’«::MDP, U, s, a)
    ğ’®, T, R, Î³ = ğ’«.ğ’®, ğ’«.T, ğ’«.R, ğ’«.Î³
    return R(s,a) + Î³*sum(T(s,a,sâ€²)*U(sâ€²) for sâ€² in ğ’®)
end

function lookahead(ğ’«::MDP, U::Vector, s, a)
    ğ’®, T, R, Î³ = ğ’«.ğ’®, ğ’«.T, ğ’«.R, ğ’«.Î³
    return R(s,a) + Î³*sum(T(s,a,sâ€²)*U[i] for (i,sâ€²) in enumerate(ğ’®))
end

struct ValueFunctionPolicy
    ğ’« # problem
    U # utility function
end

function greedy(ğ’«::MDP, U, s)
    u, a = _findmax(a->lookahead(ğ’«, U, s, a), ğ’«.ğ’œ)
    return (a=a, u=u)
end

function (Ï€::ValueFunctionPolicy)(s)
    return greedy(Ï€.ğ’«, Ï€.U, s).a
end
# -------------------------- Value Iteration --------------------------


function backup(ğ’«::MDP, U, s)
    return maximum(lookahead(ğ’«, U, s, a) for a in ğ’«.ğ’œ)
end


struct ValueIteration
    k_max # maximum number of iterations
end


function solve(M::ValueIteration, ğ’«::MDP)
    U = [0.0 for s in ğ’«.ğ’®]
    for k = 1:M.k_max
        U = [backup(ğ’«, U, s) for s in ğ’«.ğ’®]
    end
    return ValueFunctionPolicy(ğ’«, U)
end
M = ValueIteration(5)
P = MDP(discountFactor,state,action,T,R)
print(solve(M,P).U)